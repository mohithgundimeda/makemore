{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a41a5183-8920-4d20-986b-1d105ca7842e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6c0a82b-e450-483f-860a-0af003fedb5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbd00ddf-ecc5-4bec-a537-8716da9de6c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no of gpu': 1, 'cuda-available': True, 'cuda-version': '12.1'}\n"
     ]
    }
   ],
   "source": [
    "def gpu_check():\n",
    "    num_gpus=torch.cuda.device_count()\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    cuda_version = torch.version.cuda if cuda_available else 'N/A'\n",
    "    return { 'no of gpu':num_gpus , 'cuda-available': cuda_available , 'cuda-version': cuda_version }\n",
    "print(gpu_check())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad744227-fd5a-4dd7-94bb-0189360c7514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#(32,25)-> (32,25,80) -> ()\n",
    "context_size = 256\n",
    "batch_size = 32 \n",
    "n_heads = 6\n",
    "n_emb = 384\n",
    "dp_threshold = 0.4\n",
    "n_layers = 6\n",
    "eval_iters = 200\n",
    "max_iterations = 4000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "795bd901-8c9c-4ef5-8a4e-9b857700765b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(file_path , train_split):\n",
    "    global vocab, vocab_size, encode , decode , stoi , itos\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read().replace('\\n', ' ')\n",
    "        vocab = sorted(list(set(data)))\n",
    "        vocab_size = len(vocab)\n",
    "        stoi={s:i for i,s in enumerate(vocab)}\n",
    "        itos={i:s for s,i in stoi.items()}\n",
    "        encode = lambda l: [stoi[ch] for ch in l]\n",
    "        decode = lambda n: ''.join(itos[i] for i in n)\n",
    "        encoded_data = torch.tensor(encode(data))\n",
    "        threshold = int((train_split/100)*len(encoded_data))\n",
    "        train_data = encoded_data[:threshold]\n",
    "        val_data = encoded_data[threshold:]\n",
    "        \n",
    "    return train_data , val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f118d103-2c1f-4b07-9706-4706676817fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train and val data 1003854 , 111540\n"
     ]
    }
   ],
   "source": [
    "train_data , val_data = get_data('input.txt' , train_split=90)\n",
    "print(f'size of train and val data {str(len(train_data)):5s} , {str(len(val_data))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2e4d9bf-155d-49c0-9b1b-6d87040e5361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data_split = train_data if split=='train' else val_data\n",
    "    ix = torch.randint(len(data_split) - context_size , (batch_size,))\n",
    "    x = torch.stack([data_split[i:i+context_size] for i in ix])\n",
    "    y = torch.stack([data_split[i+1:i+context_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8cfcbce-c437-4e8d-a8c4-ee5eae9ac477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    '''single self-attention block'''\n",
    "    def __init__(self , head_size):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(n_emb , head_size)\n",
    "        self.k = nn.Linear(n_emb , head_size)\n",
    "        self.v = nn.Linear(n_emb , head_size)\n",
    "        #self.tril = torch.tril(torch.ones(context_size ,context_size))\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_size ,context_size)))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        query = self.q(x) # It is called self attention because the information is extracting from input(x) only\n",
    "        key = self.k(x)\n",
    "        value = self.v(x)\n",
    "        w = query @ key.transpose(-2 , -1)* key.shape[-1]**-0.5\n",
    "        wei = w.masked_fill(self.tril[:T, :T] == 0 , float('-inf')) # it will become an encoder if this step is removed\n",
    "        wei = torch.nn.functional.softmax(wei , dim =-1)\n",
    "        out = wei @ value\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14603a51-afe7-4356-b52b-17c88019c921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' multiple self-attention blocks'''\n",
    "    def __init__(self , head_size , n_heads):\n",
    "        super().__init__()\n",
    "        self.sa = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.Linear = nn.Linear(head_size*n_heads , n_emb)\n",
    "        self.dropout = nn.Dropout(dp_threshold)\n",
    "        \n",
    "    def forward(self , x):\n",
    "        sa = torch.cat([h(x) for h in self.sa], dim=-1)\n",
    "        out = self.Linear(sa)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db439dc-7843-4edd-9ea7-994cb186a93a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self , n_embd):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(\n",
    "                                nn.Linear(n_embd , 4*n_embd),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(4*n_embd , n_embd),\n",
    "                                nn.Dropout(dp_threshold))\n",
    "    def forward(self , x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bb8a2d0-a57b-43e2-97d6-554d69a39e66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self , n_emb , n_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_emb // n_heads\n",
    "        self.self_attention = MultiHeadAttention(head_size , n_heads)\n",
    "        self.ff = FeedForward(n_emb)\n",
    "        self.ly1=nn.LayerNorm(n_emb)\n",
    "        self.ly2=nn.LayerNorm(n_emb)\n",
    "    def forward(self , x):\n",
    "        x = x + self.self_attention(self.ly1(x))\n",
    "        x = x + self.ff(self.ly2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74caf2c8-4236-4e4f-97bf-8aea6dff6c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size , n_emb )\n",
    "        self.pos_embedding = nn.Embedding(context_size , n_emb)\n",
    "        self.block = nn.Sequential(*[Block(n_emb , n_heads) for _ in range(n_layers)])\n",
    "        self.Linear = nn.Linear(n_emb , vocab_size)\n",
    "        self.ly = nn.LayerNorm(n_emb)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "    def forward(self , idx , target = None):\n",
    "        b,t = idx.shape\n",
    "        \n",
    "        token_emb = self.embedding(idx) # (b,t,n_emb)\n",
    "       \n",
    "        pos_emb = self.pos_embedding(torch.arange(t , device=device)) #(t,n_emb) , think about pos_emb as a value-matrix of context char's being in that position in context\n",
    "       \n",
    "        x = token_emb + pos_emb # (b,t,n_emb)\n",
    "       \n",
    "        x = self.block(x)\n",
    "        \n",
    "        x = self.ly(x)\n",
    "        logits = self.Linear(x)\n",
    "        \n",
    "        if target is not None:\n",
    "            b,t,c = logits.shape\n",
    "            logits = logits.view(b*t , c)\n",
    "            \n",
    "            target = target.view(b*t)\n",
    "\n",
    "            loss = nn.functional.cross_entropy(logits , target)\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        return logits , loss\n",
    "        \n",
    "        \n",
    "    def generate(self , idx , max_tokens):\n",
    "        \n",
    "        for _ in range(max_tokens):\n",
    "            idx_cond = idx[: , -context_size:]\n",
    "           \n",
    "            logs ,loss = self(idx_cond)\n",
    "            \n",
    "            logs = logs[: , -1 , :]\n",
    "            \n",
    "            exps = nn.functional.softmax(logs , dim=-1)\n",
    "            ix = torch.multinomial(exps , num_samples=1)\n",
    "            idx = torch.cat((idx,ix), dim =1)\n",
    "            \n",
    "        return idx\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b0f494d-e758-44ca-915a-946f6a2647d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10795072\n"
     ]
    }
   ],
   "source": [
    "model = GPTModel()\n",
    "m=model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters()))\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2357e515-74e9-4f4d-901d-0f7f0685d8ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69684857-b186-4543-b9a8-7c81f6351865",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2370, val loss 4.2347\n",
      "step 1000: train loss 1.6142, val loss 1.8293\n",
      "step 2000: train loss 1.3978, val loss 1.6304\n",
      "step 3000: train loss 1.2881, val loss 1.5690\n",
      "step 3999: train loss 1.2085, val loss 1.5330\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_iterations):\n",
    "    \n",
    "    if i%1000 == 0 or i == max_iterations-1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        \n",
    "    x , y = get_batch('train')\n",
    "    \n",
    "    logits , loss = model(x,y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6065f275-2a0e-41f5-af74-8e11f1560f2e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " napsia and a moceful Scark for this sun's no might, And twice prove banks it backles for think'd To bank forthmer to't. Once thangman, not but misters here! My svenges and scorn from thy soon augh; I'll seize never to Edwards, With she men makes, grew to fight and die; Lest more to soon. Outh, or such as nothing no winder a king, So think or it?  ROMEO: I pray not do't.  PRINCE: Distress, Though his nature, and bapting asign.  HASTINGS: And brake I hope.  Nurse: And Paunt up you, good in where dares air.  KING RICHARD III: Can my wants Have know in all three. I have no leness some shabe the king: Nay, nor I Utoo nothing he not; therefore then to, The execution we have found heep him and present has.  KING HENRY VI: Sworn so offend with him pains, That broud How that him said, Time Mowcures the wirried his brief; Love amaid Sweetham! Sir Have shall help me, from not; And you so no loyal means you plant in From the might ragon, So not villain: such he gold be with you? O whom, will you to The few-morn.  JULIET: Come, old you poor doth, and show'd up; As fvow housepter As do her! that Apasso, as yond a spirite.  DUCHESS OF YORK: Ay, iffer, Sivowdy first; these city soven demains well When years and servel me, the faults oft are And thou happy trage melanchondest upon that I'll know thee as look'd in it ofhily.  QUEEN MARGARET: This is thy face but as doination say, Marcius he's issue; stead upon a light and knee, I am but a termost dead, Pur villain choose.  QUEEN MARGARET: For thou seomaten'dst to justposed thee; And they have warnifed to stime heir harms, Which we joy no guest to ise that a royal more. Thus, nor a farewel consul! how-be hapel, Now a comfortwounded flown of candter. Yet now, for our ducatains agains objects And hafter himself; And well but the girley sir! Let's deep'd, my good sons! Doth to them made of my grace in take of our Montague.  PRINCE MONTAGU: Is't take unvening consent? Good quounced lad, still not slow to last The doth thoughts. I'll the ghose at my soulder's excleed. I'll say, 'tis not o'er no thy nirthle: but only not him: luckle't it we glory.' But, thou, thought it comes. Thou, wortney, more, God majurely to Marcius. Or injury, tumberland, still the containment an ten time that Forth woman, if it beremoved by Countain that, despair seture that to mine. Juliet by my wantitue alone-liking on ewl, Love me awhile foot down with government? And I prison at upon the sure of just'? My covide ady that is not: The assing my virtue season from as to me? Camillo, where, Take have as onswer so fredeen hand, Ratirets of a secalles Dies, lord of Dears, shall I nort, shalls, pent go it. But not her: if you'll can meet hearns the years; My afford I stay my this, Havanley served, gentreate; With a quedicing shallob and cords their time Tybalt minds: four means, from in the house as it done--your ment, Take flesh--  MENENIUS: For the thousand of alove word,--Romeo, Art thou life, and know thee helps and state CoriPale, Unto the ease of pinces told At gentle known, and not: This may ne'er when he was, mother, well botch boy; and mistophers May law torth, As he die upon the stander of Laen of only as he that, be teach. His deputy'd limbiroacherous! Lady How cannot faithful be! My chanced Sirrah Juliet.  GirLOUCESTER: Scapes and tusp thy contempter, Where I shows thee you from that, Thou city hething on thy harse.  KING HENRY VI: And doth him stand so bbowd; Let us not kind he childest to our pack of execute Peter.  PRINCE EDWARD: His tends upon my man-liking at, No said he phazard speak cup Hermsew as like bloody shame up, and was us baded, Which, knoly full queen's blandater good Have and casented, from his prisoner To crown me.  YORK:  All Signior Provost: And that's the due weep irobate easter'd stone corth; And, o'er tears throw from the soldier, Where he fearsing behockles discontentess spertreth.  DUCKIUS: Excth, my corpose pay.  Messenger: Not with her north as rasigns me. Being graticumed me to her; They have store here weary shorn it, if Maneries of my brat; And their a tretains all be relik to a suit him.  KING RICHARD III: Faith, is queen off, nothought's? Yet back after a shunn: things, he cloves not That comest music to black with the sear?  ROMEO: Where'en, as I love The canst like in Pestingst in wars?  NOMEO: This Sons were teace indeed.  BoyETHRTESS OF YORK: In the in the queen all the right, And vengeabre it be dismortured; For then value that I senator Foult be move.  KING HENRY VI: And, let me pain infectedes; The king me scafs, Have power from my general where lie.  WARWICK: Thiever may did think.  BUCKINGHAM: Tuteshares free.  DORSET: Why, Deliek, what kill, and hecomes the more?  KING RICHARD III. L Hewake?  DUCHESS OF YORK: Of with the destire of clout, this nature their presence Wiance, when thy compans doth not thine spirit. But lets bring them once.  DUCHESS OF YORK: Dire, he does Henry: like all.  KING HENRY VI:  DUCHESS OF YORK: Welcome, gracious thundern: why then al\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_tokens=5000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18fd42e9-d6ef-4af1-a7c8-2762e115a123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path ='model.pth'\n",
    "optimizer_path ='optimizer.pth' \n",
    "torch.save(m.state_dict() , model_path)\n",
    "torch.save(optimizer.state_dict() , optimizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdeb0bb-8ec8-4798-adc8-413dd501b5c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
